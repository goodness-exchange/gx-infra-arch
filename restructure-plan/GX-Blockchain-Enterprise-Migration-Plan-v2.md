# GX Blockchain Enterprise Infrastructure Migration Plan
## Industry Best Practices Edition with Comprehensive Testing

**Document Version:** 2.0  
**Created:** December 11, 2025  
**Classification:** CONFIDENTIAL - Internal Use Only  
**Project:** GX Coin Enterprise Blockchain Platform  
**Domains:** goodness.exchange | gxcoin.money | wallet.gxcoin.money

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Current State Analysis](#2-current-state-analysis)
3. [Industry Best Practices Framework](#3-industry-best-practices-framework)
4. [Certificate Authority Architecture](#4-certificate-authority-architecture)
5. [Target Architecture](#5-target-architecture)
6. [Security & Compliance Standards](#6-security--compliance-standards)
7. [Pre-Migration Preparation](#7-pre-migration-preparation)
8. [Migration Phases](#8-migration-phases)
9. [Comprehensive Testing Plan](#9-comprehensive-testing-plan)
10. [Backup & Disaster Recovery](#10-backup--disaster-recovery)
11. [Cloudflare Integration](#11-cloudflare-integration)
12. [Post-Migration Validation](#12-post-migration-validation)
13. [Operational Runbooks](#13-operational-runbooks)

---

## 1. Executive Summary

### 1.1 Document Purpose

This document provides a comprehensive, **industry best practices-aligned** migration plan for the GX Blockchain infrastructure. It addresses:

- **High Availability (HA)** - Eliminate single points of failure
- **Load Distribution** - Balance workloads across servers
- **Environment Separation** - MainNet, TestNet, DevNet isolation
- **Security Hardening** - Certificate management, secrets, network policies
- **Disaster Recovery** - Automated backups to Google Drive
- **Comprehensive Testing** - End-to-end validation framework

### 1.2 Key Findings from Audit

#### Current CA Infrastructure (ALREADY WELL-STRUCTURED)

Your Fabric CA hierarchy follows Hyperledger best practices:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CURRENT CA HIERARCHY (fabric namespace)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                          â”‚   ca-root    â”‚ â† Root CA (Trust Anchor)         â”‚
â”‚                          â”‚   Port 7054  â”‚   Expires: Oct 2040              â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                 â”‚                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚           â”‚                     â”‚                     â”‚                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  ca-org1    â”‚       â”‚  ca-org2    â”‚       â”‚ ca-orderer  â”‚             â”‚
â”‚    â”‚  Port 9054  â”‚       â”‚  Port 10054 â”‚       â”‚  Port 8054  â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚    Issues: Org1 MSP      Issues: Org2 MSP      Issues: Orderer MSP         â”‚
â”‚    - peer certs          - peer certs          - orderer certs             â”‚
â”‚    - client certs        - client certs        - admin certs               â”‚
â”‚    - admin certs         - admin certs                                      â”‚
â”‚                                                                             â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                          â”‚   ca-tls     â”‚ â† Dedicated TLS CA               â”‚
â”‚                          â”‚  Port 11054  â”‚   (Best Practice!)               â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                          Issues all TLS certificates                        â”‚
â”‚                          Separate from identity PKI                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**âœ… Positive Findings:**
- Proper hierarchical CA structure with Root CA
- Dedicated TLS CA (industry best practice)
- Per-organization Intermediate CAs
- Certificates stored as Kubernetes Secrets
- cert-manager installed for Let's Encrypt automation
- PostgreSQL backing store for CA databases

**âš ï¸ Issues to Address:**
- All CAs running on same node (no HA)
- Root CA should be offline/HSM-protected in production
- Certificate expiry monitoring needs enhancement
- Some LoadBalancer services showing `<pending>` status

### 1.3 Infrastructure Inventory

| Server | IP | Current State | Target State |
|--------|-----|--------------|--------------|
| VPS-1 | 72.60.210.201 | Overloaded (ALL Fabric) | MainNet Node 1 + Monitoring |
| VPS-2 | 72.61.116.210 | Idle K3s Worker | MainNet Node 2 |
| VPS-3 | 72.61.81.3 | K3s Master (NO DOCKER!) | MainNet Node 3 + Backup |
| VPS-4 | 217.196.51.190 | K3s Master | DevNet + TestNet |
| VPS-5 | 195.35.36.174 | Partner + Website | Partner + Website (No change) |

---

## 2. Current State Analysis

### 2.1 Kubernetes Fabric Components (fabric namespace)

| Component | Pod Name | Status | Node | Notes |
|-----------|----------|--------|------|-------|
| **Root CA** | ca-root-0 | Running | srv1089618 | Trust anchor |
| **TLS CA** | ca-tls-0 | Running | srv1089618 | TLS certificates |
| **Orderer CA** | ca-orderer-0 | Running | srv1089618 | Orderer identity |
| **Org1 CA** | ca-org1-0 | Running | srv1089618 | Organization 1 |
| **Org2 CA** | ca-org2-0 | Running | srv1089618 | Organization 2 |
| **Orderer 0** | orderer0-0 | Running | srv1089618 | Raft leader eligible |
| **Orderer 1** | orderer1-0 | Running | srv1089618 | Raft follower |
| **Orderer 2** | orderer2-0 | Running | srv1089618 | Raft follower |
| **Orderer 3** | orderer3-0 | Running | srv1089618 | Raft follower |
| **Orderer 4** | orderer4-0 | Running | srv1089618 | Raft follower |
| **Peer0-Org1** | peer0-org1-0 | Running | srv1089618 | Anchor peer |
| **Peer1-Org1** | peer1-org1-0 | Running | srv1089618 | |
| **Peer0-Org2** | peer0-org2-0 | Running | srv1089618 | Anchor peer |
| **Peer1-Org2** | peer1-org2-0 | Running | srv1089618 | |
| **CouchDB** | couchdb-peer*-0 | Running | srv1089618 | State databases |
| **Chaincode** | gxtv3-chaincode-0 | Running | srv1089618 | Smart contract |

### 2.2 External Services Configuration

| Service | Type | External IP | Ports | Status |
|---------|------|-------------|-------|--------|
| orderer0-external | LoadBalancer | 72.60.210.201 | 27050, 7053 | âœ… Active |
| orderer1-external | LoadBalancer | 72.60.210.201 | 28050, 8053 | âœ… Active |
| orderer2-external | LoadBalancer | 72.60.210.201 | 29050, 9053 | âœ… Active |
| orderer3-external | LoadBalancer | 72.60.210.201 | 30050, 10053 | âœ… Active |
| orderer4-external | LoadBalancer | 72.60.210.201 | 31050, 11053 | âœ… Active |
| peer0-org1-external | LoadBalancer | 72.61.81.3, 72.60.210.201 | 37051, 39443 | âœ… Multi-IP |
| ca-org1-external | LoadBalancer | 72.60.210.201 | 7354, 17354 | âœ… Active |
| ingress-nginx-controller | LoadBalancer | `<pending>` | 80, 443 | âš ï¸ No External IP |

### 2.3 SSL/TLS Certificate Status

| Domain | Issuer | Expiry | Status | Action Required |
|--------|--------|--------|--------|-----------------|
| goodness.exchange | Let's Encrypt | Jul 23, 2025 | âš ï¸ 7 months | Auto-renew configured |
| gxcoin.money | Let's Encrypt | Jan 3, 2026 | âœ… Valid | Auto-renew configured |
| wallet.gxcoin.money | Let's Encrypt | Dec 27, 2025 | âœ… Valid | Auto-renew configured |
| Fabric Root CA | Self-signed | Oct 23, 2040 | âœ… Valid | 15-year validity |
| Org1 Admin Cert | ca-org1 | Nov 20, 2026 | âœ… Valid | 1-year validity |

---

## 3. Industry Best Practices Framework

### 3.1 Hyperledger Fabric Best Practices

| Practice | Current Status | Recommendation | Priority |
|----------|----------------|----------------|----------|
| **Orderer Distribution** | âŒ All on 1 node | Distribute across 3+ nodes | ğŸ”´ Critical |
| **Peer Distribution** | âŒ All on 1 node | 1 peer per org per node minimum | ğŸ”´ Critical |
| **Raft Quorum** | âœ… 5 orderers | Maintain 5 for 2-fault tolerance | âœ… Good |
| **Separate TLS CA** | âœ… Implemented | Already following best practice | âœ… Good |
| **CouchDB Per Peer** | âœ… Implemented | Each peer has dedicated CouchDB | âœ… Good |
| **Channel Per Use Case** | âš ï¸ Single channel | Consider multi-channel for privacy | ğŸŸ¡ Medium |
| **Private Data Collections** | Unknown | Implement for sensitive data | ğŸŸ¡ Medium |
| **Chaincode Lifecycle** | âœ… v2.0 lifecycle | Using modern chaincode management | âœ… Good |

### 3.2 Kubernetes Best Practices

| Practice | Current Status | Recommendation | Priority |
|----------|----------------|----------------|----------|
| **Pod Anti-Affinity** | âŒ Not configured | Spread critical pods across nodes | ğŸ”´ Critical |
| **Resource Limits** | âš ï¸ Partial | Define CPU/memory limits for all pods | ğŸŸ¡ Medium |
| **PodDisruptionBudgets** | âŒ Not configured | Ensure minimum availability | ğŸ”´ Critical |
| **Network Policies** | âš ï¸ Partial (kube-router) | Implement strict namespace isolation | ğŸŸ¡ Medium |
| **RBAC** | âœ… K3s default | Review and harden service accounts | ğŸŸ¡ Medium |
| **Secrets Management** | âš ï¸ K8s Secrets | Consider HashiCorp Vault for production | ğŸŸ¢ Low |
| **Horizontal Pod Autoscaler** | âŒ Not configured | Enable for backend services | ğŸŸ¢ Low |
| **Cluster Autoscaler** | N/A | Fixed node count (appropriate for now) | N/A |

### 3.3 Docker Best Practices

| Practice | Current Status | Recommendation | Priority |
|----------|----------------|----------------|----------|
| **Non-Root Containers** | âš ï¸ Mixed | Run all containers as non-root | ğŸŸ¡ Medium |
| **Read-Only Filesystems** | âŒ Not configured | Enable where possible | ğŸŸ¢ Low |
| **Image Scanning** | âŒ Not configured | Implement Trivy scanning | ğŸŸ¡ Medium |
| **Private Registry** | âœ… Running on :30500 | Already using private registry | âœ… Good |
| **Resource Limits** | âš ï¸ Partial | Set memory/CPU limits in docker-compose | ğŸŸ¡ Medium |
| **Health Checks** | âœ… Configured | Fabric containers have health checks | âœ… Good |
| **Log Rotation** | âš ï¸ Partial | Configure json-file log driver limits | ğŸŸ¡ Medium |

### 3.4 Certificate Authority Best Practices

| Practice | Current Status | Recommendation | Priority |
|----------|----------------|----------------|----------|
| **Root CA Offline** | âŒ Running online | Move to offline/HSM for production | ğŸŸ¡ Medium |
| **Intermediate CAs** | âœ… Implemented | Using org-specific intermediate CAs | âœ… Good |
| **Separate TLS PKI** | âœ… Implemented | TLS CA separate from identity CA | âœ… Good |
| **Certificate Rotation** | âš ï¸ Manual | Automate with cert-manager integration | ğŸŸ¡ Medium |
| **CRL/OCSP** | âš ï¸ Not verified | Implement certificate revocation | ğŸŸ¡ Medium |
| **Key Algorithm** | âœ… ECDSA P-256 | Using modern elliptic curve | âœ… Good |
| **Certificate Validity** | âš ï¸ 1 year | Appropriate, but automate renewal | ğŸŸ¡ Medium |
| **HSM Integration** | âŒ Not implemented | Consider for Root CA keys in production | ğŸŸ¢ Future |

### 3.5 Database Best Practices

| Practice | Current Status | Recommendation | Priority |
|----------|----------------|----------------|----------|
| **PostgreSQL HA** | âœ… 3 replicas | Running StatefulSet with replication | âœ… Good |
| **Redis HA** | âœ… 3 replicas | Running StatefulSet with replication | âœ… Good |
| **CouchDB Clustering** | âŒ Standalone | Consider CouchDB cluster for HA | ğŸŸ¡ Medium |
| **Automated Backups** | âœ… CronJob 6h | postgres-backup, redis-backup running | âœ… Good |
| **Connection Pooling** | âš ï¸ Unknown | Implement PgBouncer if needed | ğŸŸ¢ Low |
| **Encryption at Rest** | âš ï¸ Unknown | Verify disk encryption | ğŸŸ¡ Medium |

---

## 4. Certificate Authority Architecture

### 4.1 Current CA Deployment

```yaml
# Current CA Services in fabric namespace
Services:
  ca-root:
    ClusterIP: 10.43.63.120
    Ports: 7054 (CA), 17054 (Operations)
    External: LoadBalancer (pending) â†’ 7154, 17154
    
  ca-tls:
    ClusterIP: 10.43.170.136
    Ports: 11054 (CA), 21054 (Operations)
    External: LoadBalancer (pending) â†’ 7554, 17554
    
  ca-orderer:
    ClusterIP: 10.43.226.99
    Ports: 8054 (CA), 18054 (Operations)
    External: LoadBalancer (pending) â†’ 7254, 17254
    
  ca-org1:
    ClusterIP: 10.43.162.135
    Ports: 9054 (CA), 19054 (Operations)
    External: LoadBalancer 72.60.210.201 â†’ 7354, 17354
    
  ca-org2:
    ClusterIP: 10.43.57.59
    Ports: 10054 (CA), 20054 (Operations)
    External: LoadBalancer (pending) â†’ 7454, 17454
```

### 4.2 CA Secrets Structure

```
fabric namespace secrets:
â”œâ”€â”€ ca-root-secret (9 keys)
â”‚   â”œâ”€â”€ ca-cert.pem
â”‚   â”œâ”€â”€ ca-key.pem
â”‚   â”œâ”€â”€ tls-cert.pem
â”‚   â”œâ”€â”€ tls-key.pem
â”‚   â””â”€â”€ ... (admin credentials)
â”œâ”€â”€ ca-tls-secret (8 keys)
â”œâ”€â”€ ca-orderer-secret (8 keys)
â”œâ”€â”€ ca-org1-secret (8 keys)
â”œâ”€â”€ ca-org2-secret (8 keys)
â”œâ”€â”€ orderer0-crypto (11 keys) â† Full MSP + TLS
â”œâ”€â”€ orderer1-crypto (11 keys)
â”œâ”€â”€ orderer2-crypto (11 keys)
â”œâ”€â”€ orderer3-crypto (11 keys)
â”œâ”€â”€ orderer4-crypto (11 keys)
â”œâ”€â”€ peer0-org1-crypto (11 keys)
â”œâ”€â”€ peer0-org2-crypto (11 keys)
â”œâ”€â”€ peer1-org1-crypto (11 keys)
â”œâ”€â”€ peer1-org2-crypto (11 keys)
â”œâ”€â”€ org1-admin-crypto (6 keys)
â”œâ”€â”€ org2-admin-crypto (6 keys)
â”œâ”€â”€ chaincode-tls-client (3 keys)
â””â”€â”€ peer-tls-ca (1 key)
```

### 4.3 Target CA Architecture (Best Practices)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TARGET CA ARCHITECTURE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   PRODUCTION RECOMMENDATION:                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                      ROOT CA (OFFLINE)                               â”‚  â”‚
â”‚   â”‚   â€¢ Store private key in HSM or air-gapped system                   â”‚  â”‚
â”‚   â”‚   â€¢ Only bring online for signing intermediate CA certs             â”‚  â”‚
â”‚   â”‚   â€¢ 15-20 year validity                                             â”‚  â”‚
â”‚   â”‚   â€¢ Current: Running online (acceptable for dev/staging)            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                 â”‚                                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚                  â”‚                  â”‚                        â”‚
â”‚              â–¼                  â–¼                  â–¼                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚   ORDERER CA     â”‚ â”‚    ORG1 CA       â”‚ â”‚    ORG2 CA       â”‚           â”‚
â”‚   â”‚   VPS-1          â”‚ â”‚    VPS-1         â”‚ â”‚    VPS-2         â”‚ â† HA     â”‚
â”‚   â”‚   (Primary)      â”‚ â”‚    (Primary)     â”‚ â”‚    (Primary)     â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                        TLS CA (DEDICATED)                            â”‚  â”‚
â”‚   â”‚   â€¢ Issues ALL TLS certificates                                      â”‚  â”‚
â”‚   â”‚   â€¢ Separate trust domain from identity                             â”‚  â”‚
â”‚   â”‚   â€¢ Current: âœ… Already implemented correctly                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   CERTIFICATE TYPES:                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Type          â”‚ Issued By    â”‚ Usage                    â”‚ Validity  â”‚  â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚
â”‚   â”‚ Enrollment    â”‚ Org CAs      â”‚ Identity authentication  â”‚ 1 year    â”‚  â”‚
â”‚   â”‚ TLS Server    â”‚ TLS CA       â”‚ gRPC/HTTP TLS            â”‚ 1 year    â”‚  â”‚
â”‚   â”‚ TLS Client    â”‚ TLS CA       â”‚ Mutual TLS (mTLS)        â”‚ 1 year    â”‚  â”‚
â”‚   â”‚ Admin         â”‚ Org CAs      â”‚ Channel/chaincode admin  â”‚ 1 year    â”‚  â”‚
â”‚   â”‚ Orderer       â”‚ Orderer CA   â”‚ Ordering service         â”‚ 1 year    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4 CA Certificate Rotation Plan

```bash
#!/bin/bash
# CA Certificate Rotation Procedure

# 1. Check certificate expiry (run monthly)
check_cert_expiry() {
  kubectl get secrets -n fabric -o json | \
    jq -r '.items[] | select(.data["tls-cert.pem"]) | .metadata.name' | \
    while read secret; do
      echo "Checking: $secret"
      kubectl get secret $secret -n fabric -o jsonpath='{.data.tls-cert\.pem}' | \
        base64 -d | openssl x509 -noout -enddate
    done
}

# 2. Rotate certificates 30 days before expiry
rotate_peer_cert() {
  PEER=$1
  ORG=$2
  
  # Enroll new certificate
  fabric-ca-client enroll \
    -u https://peer0-org1:peerpw@ca-org1:9054 \
    --tls.certfiles /etc/fabric/ca-cert.pem \
    -M /tmp/new-msp
    
  # Update Kubernetes secret
  kubectl create secret generic ${PEER}-${ORG}-crypto \
    --from-file=/tmp/new-msp \
    --dry-run=client -o yaml | \
    kubectl apply -f -
    
  # Rolling restart
  kubectl rollout restart statefulset ${PEER}-${ORG} -n fabric
}

# 3. Monitor with Prometheus
# Add to prometheus-rules.yaml:
# - alert: FabricCertExpiringSoon
#   expr: fabric_cert_expiry_days < 30
#   for: 1h
#   labels:
#     severity: warning
```

---

## 5. Target Architecture

### 5.1 Server Role Distribution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TARGET SERVER DISTRIBUTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   VPS-1 (72.60.210.201) - MAINNET PRIMARY                                  â”‚
â”‚   â”œâ”€â”€ Orderer 0, 1 (Raft cluster)                                          â”‚
â”‚   â”œâ”€â”€ Peer0.Org1, Peer1.Org1                                               â”‚
â”‚   â”œâ”€â”€ CouchDB Ã— 2 (for Org1 peers)                                         â”‚
â”‚   â”œâ”€â”€ CA-Root, CA-Org1, CA-TLS                                             â”‚
â”‚   â”œâ”€â”€ Prometheus, Grafana, AlertManager                                    â”‚
â”‚   â”œâ”€â”€ Loki (log aggregation)                                               â”‚
â”‚   â””â”€â”€ Frontend (gx-wallet-frontend) - existing                             â”‚
â”‚                                                                             â”‚
â”‚   VPS-2 (72.61.116.210) - MAINNET SECONDARY                                â”‚
â”‚   â”œâ”€â”€ Orderer 2, 3 (Raft cluster)                                          â”‚
â”‚   â”œâ”€â”€ Peer0.Org2, Peer1.Org2                                               â”‚
â”‚   â”œâ”€â”€ CouchDB Ã— 2 (for Org2 peers)                                         â”‚
â”‚   â”œâ”€â”€ CA-Org2, CA-Orderer                                                  â”‚
â”‚   â”œâ”€â”€ Backend services replica                                              â”‚
â”‚   â””â”€â”€ PostgreSQL replica                                                    â”‚
â”‚                                                                             â”‚
â”‚   VPS-3 (72.61.81.3) - MAINNET TERTIARY + BACKUP                           â”‚
â”‚   â”œâ”€â”€ Orderer 4 (Raft cluster - tiebreaker)                                â”‚
â”‚   â”œâ”€â”€ PostgreSQL Primary                                                    â”‚
â”‚   â”œâ”€â”€ Redis Primary                                                         â”‚
â”‚   â”œâ”€â”€ MinIO (backup storage)                                               â”‚
â”‚   â”œâ”€â”€ Velero (K8s backup)                                                  â”‚
â”‚   â””â”€â”€ REQUIRES: Docker installation                                         â”‚
â”‚                                                                             â”‚
â”‚   VPS-4 (217.196.51.190) - DEVELOPMENT + TESTING                           â”‚
â”‚   â”œâ”€â”€ DevNet (isolated):                                                    â”‚
â”‚   â”‚   â””â”€â”€ Solo orderer, 1 peer, 1 CA, 1 CouchDB                           â”‚
â”‚   â”œâ”€â”€ TestNet (current fabric-testnet namespace)                           â”‚
â”‚   â”‚   â””â”€â”€ 3 orderers, 2 peers, full backend                               â”‚
â”‚   â”œâ”€â”€ CI/CD (Jenkins/GitLab Runner)                                        â”‚
â”‚   â””â”€â”€ Private Registry (existing :30500)                                   â”‚
â”‚                                                                             â”‚
â”‚   VPS-5 (195.35.36.174) - PARTNER + WEBSITE (NO CHANGES)                   â”‚
â”‚   â”œâ”€â”€ Partner Peer (gx-partnerorg1 namespace)                              â”‚
â”‚   â”œâ”€â”€ Partner CouchDB                                                       â”‚
â”‚   â””â”€â”€ Marketing Website (gxcoin.money)                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Kubernetes Node Affinity Configuration

```yaml
# Node Labels (to be applied)
# VPS-1
kubectl label node srv1089618.hstgr.cloud \
  topology.gx.io/zone=zone-a \
  node.gx.io/role=mainnet-primary \
  fabric.gx.io/org=org1

# VPS-2
kubectl label node srv1117946.hstgr.cloud \
  topology.gx.io/zone=zone-b \
  node.gx.io/role=mainnet-secondary \
  fabric.gx.io/org=org2

# VPS-3
kubectl label node srv1092158.hstgr.cloud \
  topology.gx.io/zone=zone-c \
  node.gx.io/role=mainnet-tertiary \
  fabric.gx.io/role=backup

# VPS-4
kubectl label node srv1089624.hstgr.cloud \
  topology.gx.io/zone=zone-d \
  node.gx.io/role=development \
  fabric.gx.io/env=dev-test
```

### 5.3 Pod Anti-Affinity Rules

```yaml
# Orderer Anti-Affinity (spread across nodes)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: orderer0
  namespace: fabric
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - orderer
            topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node.gx.io/role
                operator: In
                values:
                - mainnet-primary
                - mainnet-secondary
                - mainnet-tertiary
```

### 5.4 PodDisruptionBudget Configuration

```yaml
# Ensure minimum orderer availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: orderer-pdb
  namespace: fabric
spec:
  minAvailable: 3  # Maintain quorum (3 of 5)
  selector:
    matchLabels:
      app: orderer

---
# Ensure minimum peer availability per org
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: peer-org1-pdb
  namespace: fabric
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: peer
      org: org1

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: peer-org2-pdb
  namespace: fabric
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: peer
      org: org2
```

---

## 6. Security & Compliance Standards

### 6.1 Network Security

```yaml
# Network Policy - Fabric Namespace Isolation
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: fabric-isolation
  namespace: fabric
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow from same namespace
  - from:
    - namespaceSelector:
        matchLabels:
          name: fabric
  # Allow from backend-mainnet
  - from:
    - namespaceSelector:
        matchLabels:
          name: backend-mainnet
  # Allow from ingress
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  # Allow from partner
  - from:
    - ipBlock:
        cidr: 195.35.36.174/32
    ports:
    - protocol: TCP
      port: 7050  # Orderer
  egress:
  # Allow DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  # Allow same namespace
  - to:
    - namespaceSelector:
        matchLabels:
          name: fabric
  # Allow external orderer communication
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
    ports:
    - protocol: TCP
      port: 7050
```

### 6.2 Secrets Management

```yaml
# Current: Kubernetes Secrets (acceptable for staging)
# Recommended for Production: HashiCorp Vault

# Secret encryption at rest (verify K3s config)
# /etc/rancher/k3s/config.yaml should include:
# secrets-encryption: true

# Secret audit logging
# Enable Kubernetes audit logging for secret access
```

### 6.3 TLS Configuration Standards

```yaml
# Minimum TLS version: 1.2
# Recommended cipher suites for Fabric:
ORDERER_GENERAL_TLS_CIPHER_SUITES: |
  TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
  TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
  TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
  TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
```

---

## 7. Pre-Migration Preparation

### 7.1 Install Docker on VPS-3

```bash
#!/bin/bash
# Run on 72.61.81.3

# Install Docker CE
dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
dnf install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# Configure Docker daemon
cat > /etc/docker/daemon.json << 'EOF'
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "3"
  },
  "storage-driver": "overlay2",
  "live-restore": true,
  "default-ulimits": {
    "nofile": {
      "Name": "nofile",
      "Hard": 65536,
      "Soft": 65536
    }
  }
}
EOF

# Start Docker
systemctl enable --now docker

# Verify
docker --version
docker info
```

### 7.2 Backup to Google Drive

```bash
#!/bin/bash
# Backup Script with Google Drive Integration
# Credentials: gxc@handsforeducation.org

# Install rclone
curl https://rclone.org/install.sh | bash

# Configure rclone for Google Drive
rclone config create gdrive drive \
  scope drive \
  service_account_file /root/.config/gcloud/service-account.json

# Or interactive setup:
# rclone config
# Choose: Google Drive
# Client ID: (leave blank for shared)
# Client Secret: (leave blank for shared)
# Scope: 1 (full access)
# Root folder ID: (leave blank)
# Service Account: gxc@handsforeducation.org

# Full backup function
backup_to_gdrive() {
  BACKUP_DATE=$(date +%Y%m%d-%H%M%S)
  BACKUP_DIR="/root/backups/$BACKUP_DATE"
  mkdir -p $BACKUP_DIR
  
  echo "=== Starting Full Backup: $BACKUP_DATE ==="
  
  # 1. Kubernetes resources
  echo "Backing up Kubernetes resources..."
  for ns in fabric fabric-testnet backend-mainnet backend-testnet; do
    kubectl get all -n $ns -o yaml > $BACKUP_DIR/k8s-$ns-all.yaml
    kubectl get secrets -n $ns -o yaml > $BACKUP_DIR/k8s-$ns-secrets.yaml
    kubectl get configmaps -n $ns -o yaml > $BACKUP_DIR/k8s-$ns-configmaps.yaml
    kubectl get pvc -n $ns -o yaml > $BACKUP_DIR/k8s-$ns-pvc.yaml
  done
  
  # 2. Fabric crypto materials
  echo "Backing up Fabric crypto materials..."
  kubectl get secrets -n fabric -o json | jq -r '.items[] | select(.metadata.name | contains("crypto")) | @base64' > $BACKUP_DIR/fabric-crypto-secrets.b64
  
  # 3. PostgreSQL database
  echo "Backing up PostgreSQL..."
  kubectl exec -n backend-mainnet postgres-0 -- pg_dumpall -U postgres > $BACKUP_DIR/postgres-full.sql
  
  # 4. Redis data
  echo "Backing up Redis..."
  kubectl exec -n backend-mainnet redis-0 -- redis-cli BGSAVE
  sleep 5
  kubectl cp backend-mainnet/redis-0:/data/dump.rdb $BACKUP_DIR/redis-dump.rdb
  
  # 5. CouchDB state databases
  echo "Backing up CouchDB..."
  for peer in peer0-org1 peer0-org2 peer1-org1 peer1-org2; do
    COUCHDB_POD=$(kubectl get pod -n fabric -l app=couchdb,peer=$peer -o jsonpath='{.items[0].metadata.name}')
    kubectl exec -n fabric $COUCHDB_POD -- curl -s http://localhost:5984/_all_dbs | jq -r '.[]' | while read db; do
      kubectl exec -n fabric $COUCHDB_POD -- curl -s "http://localhost:5984/$db/_all_docs?include_docs=true" > "$BACKUP_DIR/couchdb-$peer-$db.json"
    done
  done
  
  # 6. Docker volumes (VPS-1)
  echo "Backing up Docker volumes..."
  ssh root@72.60.210.201 "cd /var/lib/docker/volumes && tar -czf /tmp/docker-volumes.tar.gz ."
  scp root@72.60.210.201:/tmp/docker-volumes.tar.gz $BACKUP_DIR/
  
  # 7. Application configs
  echo "Backing up application configs..."
  scp -r root@72.60.210.201:/home/sugxcoin/prod-blockchain/gx-coin-fabric/network/*.yaml $BACKUP_DIR/
  scp root@72.60.210.201:/home/sugxcoin/prod-blockchain/gx-protocol-backend/.env $BACKUP_DIR/backend-env
  scp root@72.60.210.201:/home/sugxcoin/prod-blockchain/gx-wallet-frontend/.env.local $BACKUP_DIR/frontend-env
  
  # 8. Create archive
  echo "Creating archive..."
  tar -czvf /root/backups/gx-full-backup-$BACKUP_DATE.tar.gz -C $BACKUP_DIR .
  
  # 9. Upload to Google Drive
  echo "Uploading to Google Drive..."
  rclone copy /root/backups/gx-full-backup-$BACKUP_DATE.tar.gz gdrive:GX-Backups/
  
  # 10. Cleanup local (keep last 3)
  ls -t /root/backups/gx-full-backup-*.tar.gz | tail -n +4 | xargs -r rm
  rm -rf $BACKUP_DIR
  
  echo "=== Backup Complete: gx-full-backup-$BACKUP_DATE.tar.gz ==="
  echo "Uploaded to: Google Drive â†’ GX-Backups/"
}

# Run backup
backup_to_gdrive
```

### 7.3 Pre-Migration Checklist

```markdown
## Pre-Migration Checklist

### Infrastructure
- [ ] Docker installed on VPS-3 (72.61.81.3)
- [ ] All K3s nodes healthy (`kubectl get nodes`)
- [ ] All pods running (`kubectl get pods -A`)
- [ ] Disk space > 50% free on all servers
- [ ] Memory usage < 80% on all servers

### Backups
- [ ] Full backup completed to Google Drive
- [ ] PostgreSQL backup verified (can restore)
- [ ] Redis backup verified
- [ ] CouchDB backup verified
- [ ] Fabric crypto materials backed up
- [ ] Kubernetes resources exported

### Network
- [ ] Inter-server connectivity tested (ping, nc)
- [ ] Required ports open on firewall
- [ ] DNS records verified
- [ ] Cloudflare configuration documented

### Security
- [ ] SSL certificates valid (> 30 days)
- [ ] Kubernetes secrets encrypted
- [ ] Access credentials documented (secure storage)

### Communication
- [ ] Stakeholders notified of maintenance window
- [ ] Rollback plan documented
- [ ] Support contacts available
```

---

## 8. Migration Phases

### Phase 1: Preparation (2-3 hours)

| Step | Action | Command/Procedure | Validation |
|------|--------|-------------------|------------|
| 1.1 | Create full backup | `backup_to_gdrive` | Check Google Drive |
| 1.2 | Install Docker on VPS-3 | See Section 7.1 | `docker info` |
| 1.3 | Label K8s nodes | See Section 5.2 | `kubectl get nodes --show-labels` |
| 1.4 | Verify connectivity | See Section 9.1 | All tests pass |

### Phase 2: Infrastructure Setup (4-6 hours)

| Step | Action | Details | Validation |
|------|--------|---------|------------|
| 2.1 | Apply PodDisruptionBudgets | Section 5.4 | `kubectl get pdb -n fabric` |
| 2.2 | Create NetworkPolicies | Section 6.1 | `kubectl get networkpolicy -n fabric` |
| 2.3 | Update node affinity | Section 5.3 | Pods scheduled correctly |
| 2.4 | Deploy additional CouchDB | VPS-2 CouchDB for Org2 | `kubectl get pods -n fabric` |

### Phase 3: Fabric Migration (6-8 hours)

| Step | Action | Risk Level | Rollback |
|------|--------|------------|----------|
| 3.1 | Scale orderers to target nodes | Low | Scale back |
| 3.2 | Migrate Org2 peers to VPS-2 | Medium | Keep original |
| 3.3 | Verify Raft consensus | Critical | Check leader election |
| 3.4 | Update anchor peers | Low | Revert config |
| 3.5 | Test chaincode | Critical | Use original peers |

### Phase 4: Cutover (2-3 hours)

| Step | Action | Downtime | Validation |
|------|--------|----------|------------|
| 4.1 | Update backend config | 0 | Rolling restart |
| 4.2 | Update ingress | ~1 min | HTTP check |
| 4.3 | Verify transactions | 0 | Submit test tx |
| 4.4 | Update partner connection | ~5 min | Partner peer sync |

### Phase 5: Validation & Monitoring (Ongoing)

| Step | Action | Frequency | Alert Threshold |
|------|--------|-----------|-----------------|
| 5.1 | Monitor pod health | Continuous | Any crash |
| 5.2 | Check block height sync | Every 5 min | > 10 block lag |
| 5.3 | Verify transaction latency | Every 1 min | > 5 seconds |
| 5.4 | Test failover | After 24 hours | Any failure |

---

## 9. Comprehensive Testing Plan

### 9.1 Network Connectivity Tests

```bash
#!/bin/bash
# /root/test-scripts/test-network-connectivity.sh

echo "=========================================="
echo "NETWORK CONNECTIVITY TEST SUITE"
echo "=========================================="
echo "Date: $(date)"
echo ""

SERVERS=(
  "72.60.210.201:VPS-1"
  "72.61.116.210:VPS-2"
  "72.61.81.3:VPS-3"
  "217.196.51.190:VPS-4"
  "195.35.36.174:VPS-5"
)

# Test 1: Basic connectivity (ICMP)
echo "=== TEST 1: ICMP Ping ==="
for server in "${SERVERS[@]}"; do
  IP=$(echo $server | cut -d: -f1)
  NAME=$(echo $server | cut -d: -f2)
  if ping -c 3 -W 2 $IP > /dev/null 2>&1; then
    echo "âœ… $NAME ($IP): Reachable"
  else
    echo "âŒ $NAME ($IP): Unreachable"
  fi
done
echo ""

# Test 2: SSH connectivity
echo "=== TEST 2: SSH (Port 22) ==="
for server in "${SERVERS[@]}"; do
  IP=$(echo $server | cut -d: -f1)
  NAME=$(echo $server | cut -d: -f2)
  if nc -zv -w 3 $IP 22 2>&1 | grep -q succeeded; then
    echo "âœ… $NAME ($IP): SSH accessible"
  else
    echo "âŒ $NAME ($IP): SSH not accessible"
  fi
done
echo ""

# Test 3: Kubernetes API
echo "=== TEST 3: Kubernetes API (Port 6443) ==="
for server in "${SERVERS[@]}"; do
  IP=$(echo $server | cut -d: -f1)
  NAME=$(echo $server | cut -d: -f2)
  if nc -zv -w 3 $IP 6443 2>&1 | grep -q succeeded; then
    echo "âœ… $NAME ($IP): K8s API accessible"
  else
    echo "âš ï¸  $NAME ($IP): K8s API not accessible (may be expected)"
  fi
done
echo ""

# Test 4: Fabric Orderer ports
echo "=== TEST 4: Fabric Orderer Ports ==="
ORDERER_PORTS=(27050 28050 29050 30050 31050)
for port in "${ORDERER_PORTS[@]}"; do
  if nc -zv -w 3 72.60.210.201 $port 2>&1 | grep -q succeeded; then
    echo "âœ… Orderer port $port: Accessible"
  else
    echo "âŒ Orderer port $port: Not accessible"
  fi
done
echo ""

# Test 5: Fabric Peer ports
echo "=== TEST 5: Fabric Peer Ports ==="
PEER_PORTS=(7051 8051 9051 10051 37051 38051 47051 48051)
for port in "${PEER_PORTS[@]}"; do
  if nc -zv -w 3 72.60.210.201 $port 2>&1 | grep -q succeeded; then
    echo "âœ… Peer port $port: Accessible"
  else
    echo "âš ï¸  Peer port $port: Not accessible (check if expected)"
  fi
done
echo ""

# Test 6: CouchDB ports
echo "=== TEST 6: CouchDB Ports ==="
COUCHDB_PORTS=(5984 6984 7984 8984)
for port in "${COUCHDB_PORTS[@]}"; do
  if curl -s -o /dev/null -w "%{http_code}" http://72.60.210.201:$port | grep -q 200; then
    echo "âœ… CouchDB port $port: Responding"
  else
    echo "âš ï¸  CouchDB port $port: Not responding (may be internal only)"
  fi
done
echo ""

# Test 7: Partner connectivity
echo "=== TEST 7: Partner Server Connectivity ==="
if nc -zv -w 3 195.35.36.174 30051 2>&1 | grep -q succeeded; then
  echo "âœ… Partner peer port 30051: Accessible"
else
  echo "âš ï¸  Partner peer port 30051: Not accessible from this node"
fi
echo ""

# Test 8: DNS resolution
echo "=== TEST 8: DNS Resolution ==="
DOMAINS=("api.gxcoin.money" "gxcoin.money" "wallet.gxcoin.money" "goodness.exchange")
for domain in "${DOMAINS[@]}"; do
  IP=$(dig +short $domain | head -1)
  if [ -n "$IP" ]; then
    echo "âœ… $domain â†’ $IP"
  else
    echo "âŒ $domain: DNS resolution failed"
  fi
done
echo ""

echo "=========================================="
echo "CONNECTIVITY TEST COMPLETE"
echo "=========================================="
```

### 9.2 Kubernetes Health Tests

```bash
#!/bin/bash
# /root/test-scripts/test-kubernetes-health.sh

echo "=========================================="
echo "KUBERNETES HEALTH TEST SUITE"
echo "=========================================="

# Test 1: Node status
echo "=== TEST 1: Node Status ==="
kubectl get nodes -o wide
echo ""
NOT_READY=$(kubectl get nodes | grep -v Ready | grep -v NAME | wc -l)
if [ "$NOT_READY" -eq 0 ]; then
  echo "âœ… All nodes are Ready"
else
  echo "âŒ $NOT_READY node(s) not Ready"
fi
echo ""

# Test 2: Pod status by namespace
echo "=== TEST 2: Pod Status ==="
for ns in fabric fabric-testnet backend-mainnet backend-testnet monitoring; do
  TOTAL=$(kubectl get pods -n $ns --no-headers 2>/dev/null | wc -l)
  RUNNING=$(kubectl get pods -n $ns --no-headers 2>/dev/null | grep Running | wc -l)
  if [ "$TOTAL" -eq "$RUNNING" ] && [ "$TOTAL" -gt 0 ]; then
    echo "âœ… $ns: $RUNNING/$TOTAL pods running"
  else
    echo "âš ï¸  $ns: $RUNNING/$TOTAL pods running"
    kubectl get pods -n $ns | grep -v Running | grep -v NAME
  fi
done
echo ""

# Test 3: Fabric pods specifically
echo "=== TEST 3: Fabric Component Status ==="
echo "Orderers:"
kubectl get pods -n fabric -l app=orderer -o wide
echo ""
echo "Peers:"
kubectl get pods -n fabric -l app=peer -o wide
echo ""
echo "CAs:"
kubectl get pods -n fabric -l app=ca -o wide
echo ""
echo "CouchDB:"
kubectl get pods -n fabric -l app=couchdb -o wide
echo ""

# Test 4: Service endpoints
echo "=== TEST 4: Service Endpoints ==="
kubectl get endpoints -n fabric
echo ""

# Test 5: PVC status
echo "=== TEST 5: Persistent Volume Claims ==="
kubectl get pvc -n fabric
PENDING_PVC=$(kubectl get pvc -n fabric | grep -v Bound | grep -v NAME | wc -l)
if [ "$PENDING_PVC" -eq 0 ]; then
  echo "âœ… All PVCs are Bound"
else
  echo "âŒ $PENDING_PVC PVC(s) not Bound"
fi
echo ""

# Test 6: Resource utilization
echo "=== TEST 6: Node Resource Utilization ==="
kubectl top nodes
echo ""

# Test 7: Pod resource utilization
echo "=== TEST 7: Pod Resource Utilization (Fabric) ==="
kubectl top pods -n fabric
echo ""

echo "=========================================="
echo "KUBERNETES HEALTH TEST COMPLETE"
echo "=========================================="
```

### 9.3 Fabric Network Tests

```bash
#!/bin/bash
# /root/test-scripts/test-fabric-network.sh

echo "=========================================="
echo "HYPERLEDGER FABRIC NETWORK TEST SUITE"
echo "=========================================="

# Set environment
export FABRIC_CFG_PATH=/etc/hyperledger/fabric
export CORE_PEER_TLS_ENABLED=true

# Test 1: Orderer health
echo "=== TEST 1: Orderer Health ==="
for i in 0 1 2 3 4; do
  PORT=$((27050 + i * 1000))
  HEALTH=$(curl -s http://72.60.210.201:$((7053 + i * 1000))/healthz 2>/dev/null)
  if [ "$HEALTH" == "OK" ] || [ -n "$HEALTH" ]; then
    echo "âœ… Orderer $i: Healthy"
  else
    echo "âŒ Orderer $i: Not responding"
  fi
done
echo ""

# Test 2: Peer health
echo "=== TEST 2: Peer Health ==="
PEER_PODS=$(kubectl get pods -n fabric -l app=peer -o jsonpath='{.items[*].metadata.name}')
for pod in $PEER_PODS; do
  STATUS=$(kubectl exec -n fabric $pod -c peer -- peer node status 2>/dev/null | grep -c STARTED)
  if [ "$STATUS" -gt 0 ]; then
    echo "âœ… $pod: Running"
  else
    echo "âŒ $pod: Not running"
  fi
done
echo ""

# Test 3: Channel membership
echo "=== TEST 3: Channel Membership ==="
PEER0_ORG1=$(kubectl get pods -n fabric -l app=peer,peer=peer0-org1 -o jsonpath='{.items[0].metadata.name}')
echo "Channels joined by $PEER0_ORG1:"
kubectl exec -n fabric $PEER0_ORG1 -c peer -- peer channel list 2>/dev/null
echo ""

# Test 4: Ledger height
echo "=== TEST 4: Ledger Height Consistency ==="
declare -A HEIGHTS
for pod in $PEER_PODS; do
  HEIGHT=$(kubectl exec -n fabric $pod -c peer -- peer channel getinfo -c gxchannel 2>/dev/null | grep -oP 'height:\K\d+')
  HEIGHTS[$pod]=$HEIGHT
  echo "$pod: Block height = $HEIGHT"
done

# Check if all heights are equal
UNIQUE_HEIGHTS=$(echo "${HEIGHTS[@]}" | tr ' ' '\n' | sort -u | wc -l)
if [ "$UNIQUE_HEIGHTS" -eq 1 ]; then
  echo "âœ… All peers have consistent ledger height"
else
  echo "âš ï¸  Ledger heights differ - may be syncing"
fi
echo ""

# Test 5: Chaincode status
echo "=== TEST 5: Chaincode Status ==="
kubectl exec -n fabric $PEER0_ORG1 -c peer -- peer lifecycle chaincode querycommitted -C gxchannel 2>/dev/null
echo ""

# Test 6: Chaincode query
echo "=== TEST 6: Chaincode Query Test ==="
QUERY_RESULT=$(kubectl exec -n fabric $PEER0_ORG1 -c peer -- peer chaincode query -C gxchannel -n gxtv3 -c '{"function":"GetMetadata","Args":[]}' 2>/dev/null)
if [ -n "$QUERY_RESULT" ]; then
  echo "âœ… Chaincode query successful"
  echo "Response: ${QUERY_RESULT:0:100}..."
else
  echo "âŒ Chaincode query failed"
fi
echo ""

# Test 7: Raft consensus
echo "=== TEST 7: Raft Consensus Status ==="
# Check orderer logs for leader election
ORDERER0_POD=$(kubectl get pods -n fabric -l app=orderer,orderer=orderer0 -o jsonpath='{.items[0].metadata.name}')
LEADER_LOG=$(kubectl logs -n fabric $ORDERER0_POD --tail=100 2>/dev/null | grep -i "leader" | tail -1)
if [ -n "$LEADER_LOG" ]; then
  echo "âœ… Raft consensus active"
  echo "Last leader event: $LEADER_LOG"
else
  echo "âš ï¸  Cannot determine Raft status from logs"
fi
echo ""

echo "=========================================="
echo "FABRIC NETWORK TEST COMPLETE"
echo "=========================================="
```

### 9.4 Application Endpoint Tests

```bash
#!/bin/bash
# /root/test-scripts/test-application-endpoints.sh

echo "=========================================="
echo "APPLICATION ENDPOINT TEST SUITE"
echo "=========================================="

# API Base URL
API_BASE="https://api.gxcoin.money"
WALLET_BASE="https://wallet.gxcoin.money"
WEBSITE_BASE="https://gxcoin.money"

# Test 1: API Health
echo "=== TEST 1: API Health Endpoints ==="
endpoints=(
  "$API_BASE/health"
  "$API_BASE/api/v1/health"
)
for endpoint in "${endpoints[@]}"; do
  STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$endpoint" 2>/dev/null)
  if [ "$STATUS" == "200" ]; then
    echo "âœ… $endpoint: HTTP $STATUS"
  else
    echo "âŒ $endpoint: HTTP $STATUS"
  fi
done
echo ""

# Test 2: API Endpoints (require auth - expect 401)
echo "=== TEST 2: API Authentication Check ==="
auth_endpoints=(
  "/api/v1/wallets"
  "/api/v1/transactions"
  "/api/v1/organizations"
  "/api/v1/balances"
  "/api/v1/admin"
)
for endpoint in "${auth_endpoints[@]}"; do
  STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$API_BASE$endpoint" 2>/dev/null)
  if [ "$STATUS" == "401" ] || [ "$STATUS" == "403" ]; then
    echo "âœ… $endpoint: Protected (HTTP $STATUS)"
  elif [ "$STATUS" == "200" ]; then
    echo "âš ï¸  $endpoint: Accessible without auth (HTTP $STATUS)"
  else
    echo "âŒ $endpoint: Error (HTTP $STATUS)"
  fi
done
echo ""

# Test 3: Website accessibility
echo "=== TEST 3: Website Accessibility ==="
websites=(
  "$WEBSITE_BASE"
  "$WALLET_BASE"
)
for site in "${websites[@]}"; do
  STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$site" 2>/dev/null)
  if [ "$STATUS" == "200" ] || [ "$STATUS" == "301" ] || [ "$STATUS" == "302" ]; then
    echo "âœ… $site: Accessible (HTTP $STATUS)"
  else
    echo "âŒ $site: Not accessible (HTTP $STATUS)"
  fi
done
echo ""

# Test 4: SSL certificate check
echo "=== TEST 4: SSL Certificate Verification ==="
domains=("api.gxcoin.money" "wallet.gxcoin.money" "gxcoin.money")
for domain in "${domains[@]}"; do
  EXPIRY=$(echo | openssl s_client -servername $domain -connect $domain:443 2>/dev/null | openssl x509 -noout -enddate 2>/dev/null | cut -d= -f2)
  if [ -n "$EXPIRY" ]; then
    echo "âœ… $domain: SSL valid until $EXPIRY"
  else
    echo "âŒ $domain: SSL certificate issue"
  fi
done
echo ""

# Test 5: Response time
echo "=== TEST 5: Response Time ==="
for site in "$API_BASE/health" "$WALLET_BASE" "$WEBSITE_BASE"; do
  TIME=$(curl -s -o /dev/null -w "%{time_total}" "$site" 2>/dev/null)
  if (( $(echo "$TIME < 2" | bc -l) )); then
    echo "âœ… $site: ${TIME}s"
  else
    echo "âš ï¸  $site: ${TIME}s (slow)"
  fi
done
echo ""

# Test 6: Backend services via kubectl
echo "=== TEST 6: Backend Service Status ==="
SERVICES=("svc-identity" "svc-admin" "svc-tokenomics" "svc-governance" "svc-organization" "svc-loanpool" "svc-tax" "projector" "outbox-submitter")
for svc in "${SERVICES[@]}"; do
  READY=$(kubectl get deployment $svc -n backend-mainnet -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
  DESIRED=$(kubectl get deployment $svc -n backend-mainnet -o jsonpath='{.spec.replicas}' 2>/dev/null)
  if [ "$READY" == "$DESIRED" ] && [ -n "$READY" ]; then
    echo "âœ… $svc: $READY/$DESIRED replicas ready"
  else
    echo "âŒ $svc: $READY/$DESIRED replicas ready"
  fi
done
echo ""

echo "=========================================="
echo "APPLICATION ENDPOINT TEST COMPLETE"
echo "=========================================="
```

### 9.5 Database Connection Tests

```bash
#!/bin/bash
# /root/test-scripts/test-database-connections.sh

echo "=========================================="
echo "DATABASE CONNECTION TEST SUITE"
echo "=========================================="

# Test 1: PostgreSQL connectivity
echo "=== TEST 1: PostgreSQL Connectivity ==="
PG_POD=$(kubectl get pods -n backend-mainnet -l app=postgres -o jsonpath='{.items[0].metadata.name}')
if kubectl exec -n backend-mainnet $PG_POD -- psql -U postgres -c "SELECT 1" > /dev/null 2>&1; then
  echo "âœ… PostgreSQL: Connected"
  
  # Check replication
  REPLICAS=$(kubectl exec -n backend-mainnet $PG_POD -- psql -U postgres -c "SELECT count(*) FROM pg_stat_replication" -t 2>/dev/null | tr -d ' ')
  echo "   Streaming replicas: $REPLICAS"
  
  # Check database size
  kubectl exec -n backend-mainnet $PG_POD -- psql -U postgres -c "SELECT pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) FROM pg_database ORDER BY pg_database_size(pg_database.datname) DESC LIMIT 5" 2>/dev/null
else
  echo "âŒ PostgreSQL: Connection failed"
fi
echo ""

# Test 2: Redis connectivity
echo "=== TEST 2: Redis Connectivity ==="
REDIS_POD=$(kubectl get pods -n backend-mainnet -l app=redis -o jsonpath='{.items[0].metadata.name}')
if kubectl exec -n backend-mainnet $REDIS_POD -- redis-cli PING 2>/dev/null | grep -q PONG; then
  echo "âœ… Redis: Connected (PONG)"
  
  # Check memory usage
  MEMORY=$(kubectl exec -n backend-mainnet $REDIS_POD -- redis-cli INFO memory 2>/dev/null | grep used_memory_human | cut -d: -f2)
  echo "   Memory usage: $MEMORY"
  
  # Check replication
  ROLE=$(kubectl exec -n backend-mainnet $REDIS_POD -- redis-cli INFO replication 2>/dev/null | grep role | cut -d: -f2)
  echo "   Role: $ROLE"
else
  echo "âŒ Redis: Connection failed"
fi
echo ""

# Test 3: CouchDB connectivity
echo "=== TEST 3: CouchDB Connectivity ==="
COUCHDB_PODS=$(kubectl get pods -n fabric -l app=couchdb -o jsonpath='{.items[*].metadata.name}')
for pod in $COUCHDB_PODS; do
  RESPONSE=$(kubectl exec -n fabric $pod -- curl -s http://localhost:5984/ 2>/dev/null)
  if echo "$RESPONSE" | grep -q "couchdb"; then
    VERSION=$(echo "$RESPONSE" | jq -r '.version' 2>/dev/null)
    echo "âœ… $pod: Connected (v$VERSION)"
  else
    echo "âŒ $pod: Connection failed"
  fi
done
echo ""

# Test 4: Database from application pods
echo "=== TEST 4: Application â†’ Database Connectivity ==="
APP_POD=$(kubectl get pods -n backend-mainnet -l app=svc-identity -o jsonpath='{.items[0].metadata.name}')
if [ -n "$APP_POD" ]; then
  # Test PostgreSQL from app
  PG_TEST=$(kubectl exec -n backend-mainnet $APP_POD -- sh -c 'nc -zv $DATABASE_HOST $DATABASE_PORT 2>&1' 2>/dev/null)
  if echo "$PG_TEST" | grep -q succeeded; then
    echo "âœ… svc-identity â†’ PostgreSQL: Connected"
  else
    echo "âŒ svc-identity â†’ PostgreSQL: Failed"
  fi
  
  # Test Redis from app
  REDIS_TEST=$(kubectl exec -n backend-mainnet $APP_POD -- sh -c 'nc -zv $REDIS_HOST $REDIS_PORT 2>&1' 2>/dev/null)
  if echo "$REDIS_TEST" | grep -q succeeded; then
    echo "âœ… svc-identity â†’ Redis: Connected"
  else
    echo "âŒ svc-identity â†’ Redis: Failed"
  fi
fi
echo ""

echo "=========================================="
echo "DATABASE CONNECTION TEST COMPLETE"
echo "=========================================="
```

### 9.6 End-to-End Transaction Test

```bash
#!/bin/bash
# /root/test-scripts/test-e2e-transaction.sh

echo "=========================================="
echo "END-TO-END TRANSACTION TEST"
echo "=========================================="

# This test submits a complete transaction flow

# Get auth token (if required)
# TOKEN=$(curl -s -X POST "$API_BASE/api/v1/auth/login" -H "Content-Type: application/json" -d '{"email":"test@example.com","password":"testpass"}' | jq -r '.token')

echo "=== Step 1: Check API Health ==="
HEALTH=$(curl -s https://api.gxcoin.money/health 2>/dev/null)
if [ -n "$HEALTH" ]; then
  echo "âœ… API is healthy"
else
  echo "âŒ API not responding"
  exit 1
fi

echo ""
echo "=== Step 2: Query Chaincode via Peer ==="
PEER_POD=$(kubectl get pods -n fabric -l app=peer,peer=peer0-org1 -o jsonpath='{.items[0].metadata.name}')
QUERY_RESULT=$(kubectl exec -n fabric $PEER_POD -c peer -- peer chaincode query -C gxchannel -n gxtv3 -c '{"function":"GetMetadata","Args":[]}' 2>/dev/null)
if [ -n "$QUERY_RESULT" ]; then
  echo "âœ… Chaincode query successful"
else
  echo "âŒ Chaincode query failed"
fi

echo ""
echo "=== Step 3: Check Block Production ==="
HEIGHT_BEFORE=$(kubectl exec -n fabric $PEER_POD -c peer -- peer channel getinfo -c gxchannel 2>/dev/null | grep -oP 'height:\K\d+')
echo "Current block height: $HEIGHT_BEFORE"

# Wait a moment for new blocks
sleep 10

HEIGHT_AFTER=$(kubectl exec -n fabric $PEER_POD -c peer -- peer channel getinfo -c gxchannel 2>/dev/null | grep -oP 'height:\K\d+')
echo "Block height after 10s: $HEIGHT_AFTER"

if [ "$HEIGHT_AFTER" -ge "$HEIGHT_BEFORE" ]; then
  echo "âœ… Blockchain is operational"
else
  echo "âš ï¸  No new blocks in 10 seconds (may be normal if no transactions)"
fi

echo ""
echo "=== Step 4: Check Event Processing ==="
PROJECTOR_POD=$(kubectl get pods -n backend-mainnet -l app=projector -o jsonpath='{.items[0].metadata.name}')
PROJECTOR_LOGS=$(kubectl logs -n backend-mainnet $PROJECTOR_POD --tail=10 2>/dev/null)
if echo "$PROJECTOR_LOGS" | grep -q -i "error"; then
  echo "âš ï¸  Projector has errors in recent logs"
else
  echo "âœ… Projector running without errors"
fi

echo ""
echo "=== Step 5: Check Outbox Processing ==="
OUTBOX_POD=$(kubectl get pods -n backend-mainnet -l app=outbox-submitter -o jsonpath='{.items[0].metadata.name}')
RESTARTS=$(kubectl get pod $OUTBOX_POD -n backend-mainnet -o jsonpath='{.status.containerStatuses[0].restartCount}')
echo "Outbox-submitter restart count: $RESTARTS"
if [ "$RESTARTS" -lt 50 ]; then
  echo "âœ… Outbox-submitter stable"
else
  echo "âš ï¸  Outbox-submitter has high restart count"
fi

echo ""
echo "=========================================="
echo "END-TO-END TEST COMPLETE"
echo "=========================================="
```

### 9.7 High Availability Failover Test

```bash
#!/bin/bash
# /root/test-scripts/test-ha-failover.sh

echo "=========================================="
echo "HIGH AVAILABILITY FAILOVER TEST"
echo "=========================================="
echo "WARNING: This test will temporarily disrupt services"
echo "Only run during maintenance windows"
echo ""

read -p "Continue with HA failover test? (yes/no): " CONFIRM
if [ "$CONFIRM" != "yes" ]; then
  echo "Test cancelled"
  exit 0
fi

# Test 1: Orderer failover
echo "=== TEST 1: Orderer Failover ==="
echo "Current orderer distribution:"
kubectl get pods -n fabric -l app=orderer -o wide

# Find current leader (if possible)
ORDERER0_POD=$(kubectl get pods -n fabric -l app=orderer,orderer=orderer0 -o jsonpath='{.items[0].metadata.name}')

echo "Scaling down orderer0..."
kubectl scale statefulset orderer0 --replicas=0 -n fabric
sleep 10

# Test if network still works
PEER_POD=$(kubectl get pods -n fabric -l app=peer,peer=peer0-org1 -o jsonpath='{.items[0].metadata.name}')
if kubectl exec -n fabric $PEER_POD -c peer -- peer channel getinfo -c gxchannel > /dev/null 2>&1; then
  echo "âœ… Network operational after orderer0 down"
else
  echo "âŒ Network failed after orderer0 down"
fi

# Restore
echo "Restoring orderer0..."
kubectl scale statefulset orderer0 --replicas=1 -n fabric
sleep 30

# Test 2: Peer failover
echo ""
echo "=== TEST 2: Peer Failover ==="
echo "Testing endorsement with peer1-org1 down..."

kubectl scale statefulset peer1-org1 --replicas=0 -n fabric
sleep 10

# Query should still work via peer0-org1
QUERY_RESULT=$(kubectl exec -n fabric $PEER_POD -c peer -- peer chaincode query -C gxchannel -n gxtv3 -c '{"function":"GetMetadata","Args":[]}' 2>/dev/null)
if [ -n "$QUERY_RESULT" ]; then
  echo "âœ… Chaincode query works with peer1-org1 down"
else
  echo "âŒ Chaincode query failed"
fi

# Restore
echo "Restoring peer1-org1..."
kubectl scale statefulset peer1-org1 --replicas=1 -n fabric
sleep 30

# Test 3: Database failover
echo ""
echo "=== TEST 3: Database Failover ==="
echo "Testing PostgreSQL replica failover..."

# This is informational - actual failover requires careful planning
echo "PostgreSQL replicas:"
kubectl get pods -n backend-mainnet -l app=postgres
echo "Replication status:"
kubectl exec -n backend-mainnet postgres-0 -- psql -U postgres -c "SELECT client_addr, state, sync_state FROM pg_stat_replication" 2>/dev/null

echo ""
echo "=========================================="
echo "HA FAILOVER TEST COMPLETE"
echo "=========================================="
```

---

## 10. Backup & Disaster Recovery

### 10.1 Google Drive Backup Configuration

```bash
#!/bin/bash
# Setup automated backup to Google Drive
# Account: gxc@handsforeducation.org

# Install rclone
curl https://rclone.org/install.sh | bash

# Create rclone config
mkdir -p ~/.config/rclone
cat > ~/.config/rclone/rclone.conf << 'EOF'
[gdrive-gx]
type = drive
scope = drive
token = {"access_token":"YOUR_TOKEN","token_type":"Bearer","refresh_token":"YOUR_REFRESH_TOKEN","expiry":"2025-12-31T00:00:00Z"}
team_drive = 
root_folder_id = 
EOF

# Alternative: Service Account (recommended)
# Upload service account JSON to /root/.config/gcloud/gx-backup-sa.json
cat > ~/.config/rclone/rclone.conf << 'EOF'
[gdrive-gx]
type = drive
scope = drive
service_account_file = /root/.config/gcloud/gx-backup-sa.json
team_drive = 
root_folder_id = GX-Infrastructure-Backups
EOF

# Test connection
rclone lsd gdrive-gx:

# Create backup directories in Google Drive
rclone mkdir gdrive-gx:GX-Backups/daily
rclone mkdir gdrive-gx:GX-Backups/weekly
rclone mkdir gdrive-gx:GX-Backups/monthly
rclone mkdir gdrive-gx:GX-Backups/pre-migration
```

### 10.2 Automated Backup CronJob

```yaml
# k8s/backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: gx-full-backup
  namespace: backend-mainnet
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-sa
          containers:
          - name: backup
            image: rclone/rclone:latest
            command:
            - /bin/sh
            - -c
            - |
              # Backup script
              DATE=$(date +%Y%m%d)
              BACKUP_DIR=/backup/$DATE
              mkdir -p $BACKUP_DIR
              
              # Export K8s resources
              kubectl get all -A -o yaml > $BACKUP_DIR/k8s-all.yaml
              kubectl get secrets -n fabric -o yaml > $BACKUP_DIR/fabric-secrets.yaml
              
              # Create tarball
              tar -czvf /backup/gx-backup-$DATE.tar.gz $BACKUP_DIR
              
              # Upload to Google Drive
              rclone copy /backup/gx-backup-$DATE.tar.gz gdrive-gx:GX-Backups/daily/
              
              # Cleanup old backups (keep 7 days)
              rclone delete gdrive-gx:GX-Backups/daily/ --min-age 7d
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
            - name: rclone-config
              mountPath: /config/rclone
            - name: gcloud-sa
              mountPath: /root/.config/gcloud
          volumes:
          - name: backup-storage
            emptyDir: {}
          - name: rclone-config
            secret:
              secretName: rclone-config
          - name: gcloud-sa
            secret:
              secretName: gcloud-service-account
          restartPolicy: OnFailure
```

### 10.3 Backup Schedule

| Type | Frequency | Retention | Storage |
|------|-----------|-----------|---------|
| **Database (PostgreSQL)** | Every 6 hours | 7 days | Local + GDrive |
| **Redis** | Every 6 hours | 7 days | Local + GDrive |
| **Fabric Crypto** | Daily | 90 days | GDrive |
| **CouchDB State** | Daily | 30 days | Local + GDrive |
| **K8s Resources** | Daily | 30 days | GDrive |
| **Full System** | Weekly | 12 weeks | GDrive |
| **Pre-Migration** | Before each change | Permanent | GDrive |

---

## 11. Cloudflare Integration

### 11.1 Current DNS Configuration (To Verify)

```bash
# Verify Cloudflare DNS records
# Login to Cloudflare dashboard or use API

# Expected DNS records for gxcoin.money:
# A     gxcoin.money         â†’ 195.35.36.174 (VPS-5 - Website)
# A     api.gxcoin.money     â†’ 72.60.210.201 (VPS-1 - API)
# A     wallet.gxcoin.money  â†’ 72.60.210.201 (VPS-1 - Wallet)
# CNAME www.gxcoin.money     â†’ gxcoin.money

# Expected DNS records for goodness.exchange:
# A     goodness.exchange    â†’ 72.60.210.201 (VPS-1)
# A     api.goodness.exchange â†’ 72.60.210.201 (VPS-1)
```

### 11.2 Cloudflare Settings Verification

```bash
#!/bin/bash
# Verify Cloudflare configuration

echo "=== Cloudflare Configuration Check ==="

# Check if using Cloudflare (orange cloud)
for domain in gxcoin.money api.gxcoin.money wallet.gxcoin.money; do
  echo "Checking $domain..."
  
  # Get IP via DNS
  DNS_IP=$(dig +short $domain)
  echo "  DNS resolves to: $DNS_IP"
  
  # Check headers for Cloudflare
  CF_CHECK=$(curl -sI https://$domain 2>/dev/null | grep -i "cf-ray\|cloudflare")
  if [ -n "$CF_CHECK" ]; then
    echo "  âœ… Cloudflare proxy ENABLED"
    echo "  Headers: $CF_CHECK"
  else
    echo "  âš ï¸  Cloudflare proxy may be DISABLED (DNS only)"
  fi
  echo ""
done
```

### 11.3 Recommended Cloudflare Settings

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLOUDFLARE RECOMMENDED SETTINGS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   SSL/TLS:                                                                  â”‚
â”‚   â”œâ”€â”€ Encryption Mode: Full (Strict)                                       â”‚
â”‚   â”œâ”€â”€ Always Use HTTPS: ON                                                 â”‚
â”‚   â”œâ”€â”€ Minimum TLS Version: 1.2                                             â”‚
â”‚   â””â”€â”€ Opportunistic Encryption: ON                                         â”‚
â”‚                                                                             â”‚
â”‚   Security:                                                                 â”‚
â”‚   â”œâ”€â”€ Security Level: Medium                                               â”‚
â”‚   â”œâ”€â”€ Bot Fight Mode: ON                                                   â”‚
â”‚   â”œâ”€â”€ Browser Integrity Check: ON                                          â”‚
â”‚   â””â”€â”€ Challenge Passage: 30 minutes                                        â”‚
â”‚                                                                             â”‚
â”‚   Firewall Rules (Create):                                                  â”‚
â”‚   â”œâ”€â”€ Block countries (optional for enterprise)                            â”‚
â”‚   â”œâ”€â”€ Rate limiting: 100 requests/minute per IP                           â”‚
â”‚   â””â”€â”€ WAF: OWASP Core Ruleset (if available)                              â”‚
â”‚                                                                             â”‚
â”‚   Caching:                                                                  â”‚
â”‚   â”œâ”€â”€ Caching Level: Standard                                              â”‚
â”‚   â”œâ”€â”€ Browser Cache TTL: 4 hours                                           â”‚
â”‚   â””â”€â”€ Always Online: ON                                                    â”‚
â”‚                                                                             â”‚
â”‚   Network:                                                                  â”‚
â”‚   â”œâ”€â”€ HTTP/2: ON                                                           â”‚
â”‚   â”œâ”€â”€ HTTP/3 (QUIC): ON                                                    â”‚
â”‚   â”œâ”€â”€ WebSockets: ON (for real-time features)                             â”‚
â”‚   â””â”€â”€ gRPC: ON (for Fabric peer communication)                            â”‚
â”‚                                                                             â”‚
â”‚   Page Rules:                                                               â”‚
â”‚   â”œâ”€â”€ api.gxcoin.money/*                                                   â”‚
â”‚   â”‚   â””â”€â”€ Cache Level: Bypass                                              â”‚
â”‚   â”œâ”€â”€ wallet.gxcoin.money/*                                                â”‚
â”‚   â”‚   â””â”€â”€ Security Level: High                                             â”‚
â”‚   â””â”€â”€ gxcoin.money/static/*                                                â”‚
â”‚       â””â”€â”€ Cache Level: Cache Everything, Edge TTL: 1 month                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.4 Load Balancing Configuration (If Using Cloudflare LB)

```yaml
# If using Cloudflare Load Balancing (paid feature)
Load Balancer: api.gxcoin.money
  Pool: gx-api-pool
    Origins:
      - 72.60.210.201:443 (VPS-1) - Primary
      - 72.61.116.210:443 (VPS-2) - Backup
    Health Check:
      - Path: /health
      - Interval: 60s
      - Timeout: 5s
      - Retries: 2
    Steering Policy: Random (or Geo)
```

---

## 12. Post-Migration Validation

### 12.1 Validation Checklist

```markdown
## Post-Migration Validation Checklist

### Infrastructure
- [ ] All K8s nodes in Ready state
- [ ] All pods Running without CrashLoopBackOff
- [ ] PVCs bound and accessible
- [ ] Resource usage within limits

### Fabric Network
- [ ] All 5 orderers healthy and in consensus
- [ ] All 4 peers synced (same block height)
- [ ] Chaincode responding to queries
- [ ] Channel configuration intact
- [ ] Anchor peers correctly configured

### Databases
- [ ] PostgreSQL primary accessible
- [ ] PostgreSQL replication working
- [ ] Redis master accessible
- [ ] Redis replication working
- [ ] CouchDB instances responding

### Applications
- [ ] Frontend loading correctly
- [ ] API endpoints responding
- [ ] Authentication working
- [ ] Backend services healthy
- [ ] Projector processing events
- [ ] Outbox-submitter stable

### Network
- [ ] DNS resolving correctly
- [ ] SSL certificates valid
- [ ] Cloudflare proxy active (if applicable)
- [ ] Inter-server connectivity working
- [ ] Partner peer connected

### Security
- [ ] No exposed sensitive ports
- [ ] Network policies active
- [ ] Secrets encrypted
- [ ] RBAC configured

### Backup
- [ ] Backup to Google Drive working
- [ ] Can restore from backup (tested)
- [ ] Backup schedule active
```

### 12.2 Sign-Off Requirements

| Role | Name | Signature | Date |
|------|------|-----------|------|
| Infrastructure Lead | | | |
| Development Lead | | | |
| Security Review | | | |
| Operations | | | |

---

## 13. Operational Runbooks

### 13.1 Emergency Contacts

| Role | Contact | Availability |
|------|---------|--------------|
| Infrastructure | [Your Name] | 24/7 |
| Blockchain | [Name] | Business hours |
| Database | [Name] | On-call |
| Security | [Name] | On-call |

### 13.2 Quick Reference Commands

```bash
# Check overall cluster health
kubectl get nodes && kubectl get pods -A | grep -v Running

# Check Fabric network
kubectl get pods -n fabric -l app=orderer && kubectl get pods -n fabric -l app=peer

# Check backend services
kubectl get pods -n backend-mainnet

# View logs
kubectl logs -f deployment/svc-identity -n backend-mainnet
kubectl logs -f statefulset/orderer0 -n fabric -c orderer

# Restart a deployment
kubectl rollout restart deployment/svc-identity -n backend-mainnet

# Scale a statefulset
kubectl scale statefulset peer0-org1 --replicas=0 -n fabric

# Execute into pod
kubectl exec -it orderer0-0 -n fabric -c orderer -- /bin/bash

# Check resource usage
kubectl top nodes && kubectl top pods -n fabric
```

### 13.3 Incident Response

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INCIDENT RESPONSE FLOWCHART                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   1. DETECT                                                                 â”‚
â”‚      â””â”€â”€ Alert received / User report / Monitoring                         â”‚
â”‚                                                                             â”‚
â”‚   2. ASSESS                                                                 â”‚
â”‚      â”œâ”€â”€ Is production affected? â†’ Priority 1                              â”‚
â”‚      â”œâ”€â”€ Is data at risk? â†’ Priority 1                                     â”‚
â”‚      â””â”€â”€ Is it a performance issue? â†’ Priority 2                           â”‚
â”‚                                                                             â”‚
â”‚   3. COMMUNICATE                                                            â”‚
â”‚      â”œâ”€â”€ Notify stakeholders                                               â”‚
â”‚      â”œâ”€â”€ Update status page (if applicable)                                â”‚
â”‚      â””â”€â”€ Create incident ticket                                            â”‚
â”‚                                                                             â”‚
â”‚   4. MITIGATE                                                               â”‚
â”‚      â”œâ”€â”€ If single component: Restart/scale                                â”‚
â”‚      â”œâ”€â”€ If network-wide: Check orderer consensus                          â”‚
â”‚      â””â”€â”€ If data corruption: Restore from backup                           â”‚
â”‚                                                                             â”‚
â”‚   5. RESOLVE                                                                â”‚
â”‚      â”œâ”€â”€ Apply fix                                                         â”‚
â”‚      â”œâ”€â”€ Verify services restored                                          â”‚
â”‚      â””â”€â”€ Run validation tests                                              â”‚
â”‚                                                                             â”‚
â”‚   6. REVIEW                                                                 â”‚
â”‚      â”œâ”€â”€ Document root cause                                               â”‚
â”‚      â”œâ”€â”€ Update runbooks                                                   â”‚
â”‚      â””â”€â”€ Implement preventive measures                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Appendix A: Glossary

| Term | Definition |
|------|------------|
| **Raft** | Consensus algorithm used by Fabric orderers |
| **MSP** | Membership Service Provider - identity management |
| **CouchDB** | NoSQL database used for Fabric state |
| **Anchor Peer** | Peer that facilitates cross-org communication |
| **Chaincode** | Smart contract in Hyperledger Fabric |
| **Endorsement** | Peer validation of transaction proposal |
| **K3s** | Lightweight Kubernetes distribution |
| **MetalLB** | Bare-metal load balancer for Kubernetes |

## Appendix B: File Locations

| Item | Location |
|------|----------|
| Fabric crypto | /home/sugxcoin/prod-blockchain/gx-coin-fabric/network/organizations |
| Backend code | /home/sugxcoin/prod-blockchain/gx-protocol-backend |
| Frontend code | /home/sugxcoin/prod-blockchain/gx-wallet-frontend |
| K8s manifests | /root/fabric-k8s/ |
| Test scripts | /root/test-scripts/ |
| Backup scripts | /root/backup-scripts/ |
| Audit results | /root/infrastructure-audit-* |

---

## Document Approval

| Version | Date | Author | Approved By |
|---------|------|--------|-------------|
| 1.0 | Dec 11, 2025 | Claude AI | Pending |
| 2.0 | Dec 11, 2025 | Claude AI | Pending |

---

**END OF DOCUMENT**
